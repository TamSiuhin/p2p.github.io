<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Instant Personalized Large Language Model Adaptation via Hypernetwork.">
  <meta name="keywords" content="Large Language Models, Hypernetwork, Personalization, LLM Adaptation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Instant Personalized Large Language Model Adaptation via Hypernetwork</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/nd.gif">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div> -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Instant Personalized Large Language Model Adaptation via Hypernetwork</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhaoxuan.info/">Zhaoxuan Tan</a><sup>1†</sup>,</span>
            <span class="author-block">
              <a href="https://zhangzx-uiuc.github.io/">Zixuan Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://haoyangwen.com/">Haoyang Wen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hsqmlzno1.github.io/">Zheng Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://rongzhizhang.org/">Rongzhi Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=9sOFHvcAAAAJ&hl=en">Pei Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://fengranmark.github.io/">Fengran Mo</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://zheyuanliu.github.io/">Zheyuan Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://qingkaizeng.github.io/">Qingkai Zeng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=P-mBKNYAAAAJ&hl=zh-CN">Qingyu Yin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.meng-jiang.com/">Meng Jiang</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Notre Dame, </span>
            <span class="author-block"><sup>2</sup>Amazon.com Inc., </span>
            <span class="author-block"><sup>3</sup>Université de Montréal,</span>


            <!-- <span class="author-block"><span style="color: #38b000; font-weight: bold;">ACL 2025 Main Conference</span><br> -->
            <span class="author-block"><sup>†</sup>Work done while as an intern at Amazon</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2510.16282"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper (arXiv)</span>
                </a>
              </span>
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://github.com/TamSiuhin/P2P"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./static/images/github.png" alt="Hugging Face" style="width: 1.5em; height: 1.5em; vertical-align: middle;">
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/Zhaoxuan/P2P_ckpt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/images/huggingface_logo-noborder.svg" alt="Hugging Face" style="width: 1.5em; height: 1.5em; vertical-align: middle;">
                  </span>
                  <span>Model</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Zhaoxuan/P2P_data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/images/huggingface_logo-noborder.svg" alt="Hugging Face" style="width: 1.5em; height: 1.5em; vertical-align: middle;">
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Personalized large language models (LLMs) tailor content to individual preferences using user profiles or histories. However, existing parameter-efficient fine-tuning (PEFT) methods, such as the ``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for each user, making them computationally expensive and impractical for real-time updates. We introduce Profile-to-PEFT, a scalable framework that employs a hypernetwork, trained end-to-end, to map a user's encoded profile directly to a full set of adapter parameters (e.g., LoRA), eliminating per-user training at deployment. This design enables instant adaptation, generalization to unseen users, and privacy-preserving local deployment. Experimental results demonstrate that our method outperforms both prompt-based personalization and OPPU while using substantially fewer computational resources at deployment. The framework exhibits strong generalization to out-of-distribution users and maintains robustness across varying user activity levels and different embedding backbones. The proposed Profile-to-PEFT framework enables efficient, scalable, and adaptive LLM personalization suitable for large-scale applications.          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Motivation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Motivation</h2>

        <!-- Teaser Image -->
        <div class="columns is-centered">
          <div class="column is-three-fifths">
            <div class="item">
              <img src="./static/images/teaser.png" alt="PUGC motivation illustration showing how user-generated content contains implicit preferences"/>
              <h2 class="subtitle has-text-centered">
                The "One-PEFT-Per-User" method uses computationally intensive fine-tuning to create personalizedparameters. In contrast, our proposed Profile-to-PEFT uses a hypernetwork to directly generate parameters from user history or profile in a single inference pass.
              </h2>
            </div>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            While large language models (LLMs) offer powerful "one-size-fits-all" capabilities , tailoring them to individual user preferences remains a critical research direction. Methods based on parameter-efficient fine-tuning (PEFT), particularly the "One-PEFT-Per-User" (OPPU) paradigm, have been effective in encoding user preferences into lightweight, specific parameters. Despite this success, their reliance on training a unique module from scratch for each user presents a major limitation. This approach is computationally expensive, faces significant scalability challenges in systems with millions of users, and is impractical for real-time updates as preferences evolve. This bottleneck motivates our work: to explore whether a hypernetwork can be used to learn a direct mapping from a user's profile to a full set of personalized adapter parameters , enabling instant, scalable, and efficient LLM personalization without relying on iterative, per-user fine-tuning at deployment.          </p>
        </div>
      </div>
    </div>
    
    <!--/ Motivation. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Preference Alignment Using UGC. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Profile-to-PEFT Framework</h2>
        
        <!-- Method Figure -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="item">
              <img src="./static/images/overview.png" alt="PUGC framework illustration showing the three-stage process of converting UGC to preference data"/>
              <h2 class="subtitle has-text-centered">
                Overview of the Profile-to-PEFT architecture,
                where user history, depth, module embeddings are fed
                into the hypernetwork to obtain personalized LoRA.
                P2P is optimized in a end-to-end training manner.            </div>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            Our proposed method, Profile-to-PEFT (P2P), leverages a hypernetwork to generate personalized adapter parameters instantly, enabling scalable and real-time LLM adaptation. Unlike traditional approaches like the "One-PEFT-Per-User" (OPPU) paradigm , which depend on computationally intensive fine-tuning for every individual user , P2P trains a single hypernetwork to learn a direct mapping from a user's profile to their specific parameters. This framework employs the end-to-end trained hypernetwork to take a user's encoded profile as input and directly generate a full set of adapter parameters (e.g., LoRA) in a single forward pass. This design completely eliminates the need for per-user training at deployment , making it a practical and efficient alternative that offers strong generalization to unseen users and significantly reduces computational overhead.        </div>
      </div>
    </div>
    <!--/ Preference Alignment Using UGC. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Main Experimental Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experimental Results</h2>
        
        <h3 class="title is-5">Random Split Results</h3>
        <!-- Results Figure -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="item">
              <img src="./static/images/random_result.png" alt="Main experimental results showing PUGC performance across different benchmarks and models"/>
              <h2 class="subtitle has-text-centered">
                Main experiment results on the LaMP and LongLaMP benchmarks under the Random split setting.
              </h2>
            </div>
          </div>
        </div>

        <h3 class="title is-5">Out-of-Distribution Split Results</h3>
        <!-- Results Figure -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="item">
              <img src="./static/images/OOD_result.png" alt="Main experimental results showing PUGC performance across different benchmarks and models"/>
              <h2 class="subtitle has-text-centered">
                Main experiment results on the LaMP and LongLaMP benchmarks under the OOD split setting.
              </h2>
            </div>
          </div>
        </div>
        
        <h3 class="title is-5">LLM-as-a-Judge Evaluation Results</h3>
        <!-- Results Figure -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="item">
              <img src="./static/images/llm_judge_results.png" alt="Main experimental results showing PUGC performance across different benchmarks and models" style="width: 70%; display: block; margin-left: auto; margin-right: auto;"/>
              <h2 class="subtitle has-text-centered">
                LLM-as-a-Judge (GPT-4o) evaluation on Personal Reddit and Empathic Conversation datasets.
              </h2>
            </div>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            Across the majority of tasks in the random split setting, P2P demonstrates superior or highly competitive performance. On average, it achieves the highest accuracy in classification tasks and the best ROUGE-L scores in generation tasks. Compared to the PEFT-based OPPU baseline, which requires expensive per-user training, P2P achieves better average performance in both classification and generation without any user-specific fine-tuning at deployment. The framework also demonstrates strong generalization to out-of-distribution users. In LLM-as-a-Judge evaluations on open-ended tasks, P2P not only achieves the highest average score (3.98) but also secures the best win-rate (58.4%) against the base model, confirming its superior personalization capabilities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Main Experimental Results. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Additional Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Additional Analysis</h2>
        
        <!-- First Row of Subsections -->
        <div class="columns">
          <div class="column is-half">
            <h3 class="title is-5">Training Data Diversity > Quantity</h3>
            <div class="item">
              <img src="./static/images/diversity_quantity.png" alt="Ablation study results showing the contribution of different PUGC components"/>
              <p class="content has-text-justified">
                To analyze the trade-off between data diversity and volume, we compare P2P models trained on data from a varying number of users (diversity) versus a varying number of data points per user (quantity). Results show that increasing user diversity yields a significant performance boost across all task types. In contrast, increasing the data quantity per user provides diminishing returns. This suggests that the model benefits more from exposure to a wider range of user profiles than from more data from a few users, confirming that learning a generalizable mapping is key.
              </p>
            </div>
          </div>
          
          <div class="column is-half">
            <h3 class="title is-5">Deployment Efficiency</h3>
            <div class="item">
              <img src="./static/images/deployment_time.png" alt="Performance scaling with different amounts of UGC data"/>
              <p class="content has-text-justified">
                We compare the time required to generate personalized PEFT parameters for each user at deployment On average, OPPU (LoRA) takes 20.44s per user. In contrast, our proposed P2P requires only 0.57s per user, representing a speedup of 33x. The cumulative time for OPPU increases steeply and linearly with the user count, while the cost for P2P remains near-zero and constant. This highlights P2P's suitability for large-scale, real-time applications.
              </p>
            </div>
          </div>
        </div>

        <!-- Second Row of Subsections -->
        <div class="columns">
          <div class="column is-half">
            <h3 class="title is-5">Embedding Model Robustness</h3>
            <div class="item">
              <img src="./static/images/different_emb_model.png" alt="Cross-domain evaluation results showing PUGC's generalization capabilities"/>
              <p class="content has-text-justified">
                The quality of user embeddings is critical for the hypernetwork. We test three different embedding backbones: Qwen3-Emb-4B (4B), GTE-large (0.3B), and GTE-base (0.07B). All backbones yield substantial improvements over the non-personalized baseline, indicating P2P is not overly sensitive to the embedding model. While the strongest model (Qwen3-Emb-4B) performs best, the smaller GTE models still achieve 95%-98% of its performance, demonstrating the framework's robustness and flexibility.
              </p>
            </div>
          </div>
          
          <div class="column is-half">
            <h3 class="title is-5">P2P Robust to User Activity Level</h3>
            <div class="item">
              <img src="./static/images/active_level.png" alt="Human evaluation results comparing PUGC with baseline methods"/>
              <p class="content has-text-justified">
                We analyze P2P's performance across users with varying levels of activity (i.e., different amounts of historical data). Users are grouped into 'low' (1-10 samples), 'medium' (10-50 samples), and 'high' (50+ samples) activity levels. P2P consistently outperforms the non-personalized baseline across all groups, with performance improving as user activity increases. This demonstrates the framework's robustness and its ability to effectively personalize for both new users with sparse data and highly active users with rich histories.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Additional Results. -->
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-fifths">
        <h2 class="title is-3">Online Iterative Training Can Further Improve Alignment Performance</h2>
        
        <div class="item">
          <img src="./static/images/online_iterative.png" alt="Results showing performance improvements with online iterative training" style="max-width: 70%;"/>
          <p class="content has-text-justified">
            While our main experiments focus on offline training, online iterative RLHF has demonstrated stronger performance. We evaluate PUGC in an online iterative setting by splitting UGC data into three subsets for sequential training iterations. Each iteration yields steady improvements in LC win rates (13.74%, 3.22%, and 3.44% respectively), achieving a 37.51% win rate by the third iteration compared to 35.93% in the offline setting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{tan2025instant,
      title={Instant Personalized Large Language Model Adaptation via Hypernetwork},
      author={Tan, Zhaoxuan and Zhang, Zixuan and Wen, Haoyang and Li, Zheng and Zhang, Rongzhi and Chen, Pei and Mo, Fengran and Liu, Zheyuan and Zeng, Qingkai and Yin, Qingyu and others},
      journal={arXiv preprint arXiv:2510.16282},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
